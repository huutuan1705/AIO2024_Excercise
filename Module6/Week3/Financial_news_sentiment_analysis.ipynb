{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "seed = 1\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "import unidecode\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'all-data.csv'\n",
    "headers = ['sentiment', 'content']\n",
    "df = pd.read_csv(\n",
    "    dataset_path, names=headers, encoding='ISO-8859-1'\n",
    ")\n",
    "\n",
    "classes = {\n",
    "    class_name: idx for idx, class_name in enumerate(df['sentiment'].unique().tolist())\n",
    "}\n",
    "df['sentiment'] = df['sentiment'].apply(lambda x: classes[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_stop_words = stopwords.words('english')\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def text_normalize(text):\n",
    "    text = text.lower()\n",
    "    text = unidecode.unidecode(text)\n",
    "    text = text.strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = ' '.join([word for word in text.split(' ') if word not in english_stop_words])\n",
    "    text = ' '.join([stemmer.stem(word) for word in text.split(' ')])\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: text_normalize(x))\n",
    "vocab = []\n",
    "for sentence in df['content'].tolist():\n",
    "    tokens = sentence.split()\n",
    "    for token in tokens:\n",
    "        if token not in vocab:\n",
    "            vocab.append(token)\n",
    "            \n",
    "vocab.append('UNK')\n",
    "vocab.append('PAD')\n",
    "\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "vocab_size = len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(text, word_to_idx, max_seq_len):\n",
    "    tokens = []\n",
    "    for w in text.split():\n",
    "        try:\n",
    "            w_ids = word_to_idx[w]\n",
    "        except:\n",
    "            w_ids = word_to_idx['UNK']\n",
    "        tokens.append(w_ids)\n",
    "        \n",
    "    if len(tokens) < max_seq_len:\n",
    "        tokens += [word_to_idx['PAD']] * (max_seq_len - len(tokens))\n",
    "    elif len(tokens) > max_seq_len:\n",
    "        tokens = tokens[:max_seq_len]\n",
    "        \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "text_size = 0.125\n",
    "is_shuffle = True\n",
    "\n",
    "texts = df['content'].tolist()\n",
    "labels = df['sentiment'].tolist()\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    texts, labels,\n",
    "    test_size=val_size,\n",
    "    random_state=seed,\n",
    "    shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=val_size,\n",
    "    random_state=seed,\n",
    "    shuffle=is_shuffle\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinancialNews(Dataset):\n",
    "    def __init__(self, X, y, word_to_idx, max_seq_len, transform=None):\n",
    "        self.texts = X\n",
    "        self.labels = y\n",
    "        self.word_to_idx = word_to_idx\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            text = self.transform(\n",
    "                text, self.word_to_idx, self.max_seq_len\n",
    "            )\n",
    "        text = torch.tensor(text)\n",
    "        \n",
    "        return text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = 32\n",
    "train_dataset = FinancialNews(\n",
    "    X_train, y_train, \n",
    "    word_to_idx=word_to_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    transform=transform\n",
    ")\n",
    "val_dataset = FinancialNews(\n",
    "    X_val, y_val, \n",
    "    word_to_idx=word_to_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    transform=transform\n",
    ")\n",
    "test_dataset = FinancialNews(\n",
    "    X_test, y_test, \n",
    "    word_to_idx=word_to_idx,\n",
    "    max_seq_len=max_seq_len,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_batch_size = 128\n",
    "test_batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=train_batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=test_batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, \n",
    "                 hidden_size, n_layers, n_classes, \n",
    "                 dropout_prob):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_size, n_layers, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc1 = nn.Linear(hidden_size, 16)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(16, n_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, hn = self.rnn(x)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(list(classes.keys()))\n",
    "embedding_dim = 64\n",
    "hidden_size = 64\n",
    "n_layers = 2\n",
    "dropout_prob = 0.2\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = SentimentClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    hidden_size=hidden_size,\n",
    "    n_layers=n_layers,\n",
    "    n_classes=n_classes,\n",
    "    dropout_prob=dropout_prob\n",
    ").to(device)\n",
    "\n",
    "lr = 1e-4\n",
    "epochs = 50\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    losses = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "    loss = sum(losses) / len(losses)\n",
    "    acc = correct / total\n",
    "    \n",
    "    return correct, acc\n",
    "\n",
    "def fit(model, train_loader, val_loader, criterion, optimizer, device, epochs):\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        batch_train_losses = []\n",
    "        model.train()\n",
    "        \n",
    "        for idx, (inputs, labels) in enumerate(train_loader):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_train_losses.append(loss.item())\n",
    "        \n",
    "        train_loss = sum(batch_train_losses) / len(batch_train_losses)\n",
    "        train_losses.append(train_loss)\n",
    "        \n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        \n",
    "        print (f'EPOCH { epoch + 1}:\\t Train loss :{train_loss:.4f}\\t Val loss :{val_loss:.4f}')\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\t Train loss :0.9561\t Val loss :570.0000\n",
      "EPOCH 2:\t Train loss :0.9280\t Val loss :570.0000\n",
      "EPOCH 3:\t Train loss :0.9356\t Val loss :570.0000\n",
      "EPOCH 4:\t Train loss :0.9364\t Val loss :570.0000\n",
      "EPOCH 5:\t Train loss :0.9351\t Val loss :570.0000\n",
      "EPOCH 6:\t Train loss :0.9353\t Val loss :570.0000\n",
      "EPOCH 7:\t Train loss :0.9339\t Val loss :570.0000\n",
      "EPOCH 8:\t Train loss :0.9291\t Val loss :570.0000\n",
      "EPOCH 9:\t Train loss :0.9320\t Val loss :570.0000\n",
      "EPOCH 10:\t Train loss :0.9265\t Val loss :570.0000\n",
      "EPOCH 11:\t Train loss :0.9316\t Val loss :570.0000\n",
      "EPOCH 12:\t Train loss :0.9331\t Val loss :570.0000\n",
      "EPOCH 13:\t Train loss :0.9301\t Val loss :570.0000\n",
      "EPOCH 14:\t Train loss :0.9319\t Val loss :570.0000\n",
      "EPOCH 15:\t Train loss :0.9279\t Val loss :570.0000\n",
      "EPOCH 16:\t Train loss :0.9374\t Val loss :570.0000\n",
      "EPOCH 17:\t Train loss :0.9298\t Val loss :570.0000\n",
      "EPOCH 18:\t Train loss :0.9323\t Val loss :570.0000\n",
      "EPOCH 19:\t Train loss :0.9280\t Val loss :570.0000\n",
      "EPOCH 20:\t Train loss :0.9303\t Val loss :570.0000\n",
      "EPOCH 21:\t Train loss :0.9301\t Val loss :571.0000\n",
      "EPOCH 22:\t Train loss :0.9281\t Val loss :571.0000\n",
      "EPOCH 23:\t Train loss :0.9220\t Val loss :571.0000\n",
      "EPOCH 24:\t Train loss :0.9301\t Val loss :571.0000\n",
      "EPOCH 25:\t Train loss :0.9212\t Val loss :571.0000\n",
      "EPOCH 26:\t Train loss :0.9240\t Val loss :571.0000\n",
      "EPOCH 27:\t Train loss :0.9246\t Val loss :571.0000\n",
      "EPOCH 28:\t Train loss :0.9286\t Val loss :571.0000\n",
      "EPOCH 29:\t Train loss :0.9272\t Val loss :571.0000\n",
      "EPOCH 30:\t Train loss :0.9271\t Val loss :572.0000\n",
      "EPOCH 31:\t Train loss :0.9335\t Val loss :571.0000\n",
      "EPOCH 32:\t Train loss :0.9277\t Val loss :571.0000\n",
      "EPOCH 33:\t Train loss :0.9244\t Val loss :571.0000\n",
      "EPOCH 34:\t Train loss :0.9296\t Val loss :571.0000\n",
      "EPOCH 35:\t Train loss :0.9262\t Val loss :572.0000\n",
      "EPOCH 36:\t Train loss :0.9199\t Val loss :573.0000\n",
      "EPOCH 37:\t Train loss :0.9256\t Val loss :573.0000\n",
      "EPOCH 38:\t Train loss :0.9224\t Val loss :575.0000\n",
      "EPOCH 39:\t Train loss :0.9195\t Val loss :575.0000\n",
      "EPOCH 40:\t Train loss :0.9147\t Val loss :575.0000\n",
      "EPOCH 41:\t Train loss :0.9201\t Val loss :578.0000\n",
      "EPOCH 42:\t Train loss :0.9230\t Val loss :577.0000\n",
      "EPOCH 43:\t Train loss :0.9201\t Val loss :576.0000\n",
      "EPOCH 44:\t Train loss :0.9128\t Val loss :577.0000\n",
      "EPOCH 45:\t Train loss :0.9216\t Val loss :580.0000\n",
      "EPOCH 46:\t Train loss :0.9144\t Val loss :575.0000\n",
      "EPOCH 47:\t Train loss :0.9071\t Val loss :569.0000\n",
      "EPOCH 48:\t Train loss :0.8895\t Val loss :563.0000\n",
      "EPOCH 49:\t Train loss :0.8682\t Val loss :576.0000\n",
      "EPOCH 50:\t Train loss :0.8604\t Val loss :572.0000\n"
     ]
    }
   ],
   "source": [
    "train_losses, val_losses = fit(\n",
    "    model, train_loader, val_loader, criterion, optimizer, device, epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on val / test dataset\n",
      "Val accuracy : 0.5896907216494846\n",
      "Test accuracy : 0.634020618556701\n"
     ]
    }
   ],
   "source": [
    "val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
    "\n",
    "print('Evaluation on val / test dataset')\n",
    "print('Val accuracy :', val_acc )\n",
    "print('Test accuracy :', test_acc )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
