{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clonehttps://github.com/congnghia0609/ntc-scv.git\n",
    "!unzip ./ntc-scv/data/data_test.zip -d ./data\n",
    "!unzip ./ntc-scv/data/data_train.zip -d ./data\n",
    "!rm-rf ./ntc-scv\n",
    "!pip install langid\n",
    "!pip install -q torchtext==0.16.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_data_from_path(folder_path):\n",
    "    examples = []\n",
    "    for label in os.listdir(folder_path):\n",
    "        full_path = os.path.join(folder_path, label)\n",
    "        for file_name in os.listdir(full_path):\n",
    "            file_path = os.path.join(full_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.readlines()\n",
    "            sentence = \" \".join(lines)\n",
    "            if label == \"neg\":\n",
    "                label = 0\n",
    "            if label == \"pos\":\n",
    "                label = 1\n",
    "            \n",
    "            data = {\n",
    "                'sentence': sentence,\n",
    "                'label': label\n",
    "            }\n",
    "            examples.append(data)\n",
    "    \n",
    "    return pd.DataFrame(examples)\n",
    "\n",
    "folder_paths = {\n",
    "    'train': './data/data_train/train',\n",
    "    'valid': './data/data_train/test',\n",
    "    'test': './data/data_test/test',\n",
    "}\n",
    "train_df = load_data_from_path(folder_paths['train'])\n",
    "valid_df = load_data_from_path(folder_paths['valid'])\n",
    "test_df = load_data_from_path(folder_paths['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langid.langid import LanguageIdentifier, model\n",
    "def identify_vn(df):\n",
    "    identifier = LanguageIdentifier.from_modelstring(model, norm_probs=True)\n",
    "    not_vi_idx = set()\n",
    "    THRESHOLD = 0.9\n",
    "    for idx, row in df.iterrows():\n",
    "        score = identifier.classify(row[\"sentence\"])\n",
    "        if score[0] != \"vi\" or (score[0] == \"vi\" and score[1] <= THRESHOLD):\n",
    "            not_vi_idx.add(idx)\n",
    "            \n",
    "    vi_df = df[~df.index.isin(not_vi_idx)]\n",
    "    not_vi_df = df[df.index.isin(not_vi_idx)]\n",
    "    return vi_df, not_vi_df\n",
    "\n",
    "train_df_vi, train_df_other = identify_vn(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def preprocess_text(text):\n",
    "    url_pattern = re.compile(r'https?://\\s+\\wwww\\.\\s+')\n",
    "    text = url_pattern.sub(r\" \", text)\n",
    "    \n",
    "    html_pattern = re.compile(r'<[^<>]+>')\n",
    "    text = html_pattern.sub(\" \", text)\n",
    "    \n",
    "    replace_chars = list(string.punctuation + string.digits)\n",
    "    for char in replace_chars:\n",
    "        text = text.replace(char, \" \")\n",
    "        \n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U0001F1F2-\\U0001F1F4\"\n",
    "        u\"\\U0001F1E6-\\U0001F1FF\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001f926-\\U0001f937\"\n",
    "        u\"\\U0001F1F2\"\n",
    "        u\"\\U0001F1F4\"\n",
    "        u\"\\U0001F620\"\n",
    "        u\"\\u200d\"\n",
    "        u\"\\u2640-\\u2642\"\n",
    "        \"]+\", flags=re.UNICODE\n",
    "    )\n",
    "    text = emoji_pattern.sub(r\" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text.lower()\n",
    "\n",
    "train_df_vi['preprocess_sentence'] = [\n",
    "    preprocess_text(row['sentence']) for index, row in train_df_vi.iterrows()\n",
    "]\n",
    "valid_df['preprocess_sentence'] = [\n",
    "    preprocess_text(row['sentence']) for index, row in valid_df.iterrows()\n",
    "]\n",
    "test_df['preprocess_sentence'] = [\n",
    "    preprocess_text(row['sentence']) for index, row in test_df.iterrows()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer(\"basic_english\")\n",
    "\n",
    "def yeild_tokens(sentences, tokenizer):\n",
    "    for sentence in sentences:\n",
    "        yield tokenizer(sentence)\n",
    "        \n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "voacb_size = 10000\n",
    "vocabulary = build_vocab_from_iterator(\n",
    "    yeild_tokens(train_df_vi['preprocess_sentence'], tokenizer),\n",
    "    max_tokens=voacb_size,\n",
    "    specials=[\"<pad>\", \"<unk>\"]\n",
    ")\n",
    "\n",
    "vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
    "\n",
    "from torchtext.data.functional import to_map_style_dataset\n",
    "def prepare_dataset(df):\n",
    "    for index, row in df.iterrows():\n",
    "        sentence = row['preprocess_sentence']\n",
    "        encoded_sentence = vocabulary(tokenizer(sentence))\n",
    "        label = row['label']\n",
    "        yield encoded_sentence, label\n",
    "        \n",
    "train_dataset = prepare_dataset(train_df_vi)\n",
    "train_dataset = to_map_style_dataset(train_dataset)\n",
    "\n",
    "valid_dataset = prepare_dataset(valid_df)\n",
    "valid_dataset = to_map_style_dataset(valid_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    # create inputs, offsets, labels for batch\n",
    "    encoded_sentences, labels = [], []\n",
    "    for encoded_sentence, label in batch:\n",
    "        labels.append(label)\n",
    "        encoded_sentence = torch.tensor(encoded_sentence, dtype=torch.int64)\n",
    "        encoded_sentences.append(encoded_sentence)\n",
    "\n",
    "    labels = torch.tensor(labels, dtype=torch.int64)\n",
    "    encoded_sentences = pad_sequence(\n",
    "        encoded_sentences,\n",
    "        padding_value=vocabulary[\"<pad>\"]\n",
    "    )\n",
    "\n",
    "    return encoded_sentences, labels\n",
    "\n",
    "\n",
    "batch_size = 128\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_batch\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_batch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn \n",
    "class TextCNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, kernel_sizes, num_filters, num_classes):\n",
    "        super(TextCNN, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.num_filters = num_filters\n",
    "        self.num_classes = num_classes\n",
    "        self.embedding = nn.Embedding(voacb_size, embedding_dim, padding_idx=0)\n",
    "        self.conv = nn.ModuleList([\n",
    "            nn.Conv1d(\n",
    "                in_channels=embedding_dim,\n",
    "                out_channels=num_filters,\n",
    "                kernel_size=k,\n",
    "                stride=1\n",
    "            ) for k in kernel_sizes\n",
    "        ])\n",
    "        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size, sequence_length = x.shape\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
