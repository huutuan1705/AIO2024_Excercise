{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-cat-detection\")\n",
    "print(\"Path to dataset file: \", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.resnet import ResNet50_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = self.filter_images_with_multiple_objects()\n",
    "        \n",
    "    def filter_images_with_multiple_objects(self):\n",
    "        valid_image_files = []\n",
    "        for f in os.listdir(self.image_dir):\n",
    "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
    "                img_name = f\n",
    "                annotation_name = os.path.splitext(img_name)[0] +\".xml\"\n",
    "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "                \n",
    "                # Keep iomages that have single object\n",
    "                if self.count_objects_in_annotation(annotation_path) == 1:\n",
    "                    valid_image_files.append(img_name)\n",
    "        return valid_image_files\n",
    "    \n",
    "    def count_objects_in_annotation(self, annotation_path):\n",
    "        try:\n",
    "            tree = ET.parse(annotation_path)\n",
    "            root = tree.getroot()\n",
    "            count = 0\n",
    "            for obj in root.findall(\"object\"):\n",
    "                count += 1\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img1_name = self.image_files[idx]\n",
    "        img1_path = os.path.join(self.image_dir, img1_name)\n",
    "        \n",
    "        annotation_name = os.path.splitext(img1_name)\n",
    "        img1_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name)\n",
    "        )\n",
    "        idx2 = random.randint(0, len(self.image_files) - 1)\n",
    "        img2_file = self.image_files[idx2]\n",
    "        img2_path = os.path.join(self.image_dir, img2_file)\n",
    "        \n",
    "        annotation_name = os.path.splitext(img2_file)[0] + \".xml\"\n",
    "        img2_annotations = self.parse_annotation(\n",
    "            os.path.join(self.annotations_dir, annotation_name)\n",
    "        )\n",
    "        \n",
    "        img1 = Image.open(img1_path).convert(\"RGB\")\n",
    "        img2 = Image.open(img1_path).convert(\"RGB\")\n",
    "\n",
    "        merged_image = Image.new(\"RGB\", (img1.width + img2.width, max(img1.height, img2.height)))\n",
    "        \n",
    "        merged_image.paste(img1, (0, 0))\n",
    "        merged_image.paste(img2, (img1.width, 0))\n",
    "        merged_w = img1.width + img2.width\n",
    "        merged_h = max(img1.height, img2.height)\n",
    "        \n",
    "        merged_annotations = []\n",
    "        \n",
    "        merged_annotations.append(\n",
    "            {\n",
    "                \"bbox\": img1_annotations[1].tolist(),\n",
    "                \"label\": img1_annotations[0]\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        new_bbox = [\n",
    "            (img2_annotations[1][0]*img2.width + img1.width) / merged_w,\n",
    "            img2_annotations[1][1]*img2.height/merged_h,\n",
    "            (img2_annotations[1][2]*img2.width + img1.width) / merged_w,\n",
    "            img2_annotations[1][3]*img2.height/merged_h,\n",
    "        ]\n",
    "        \n",
    "        merged_annotations.append(\n",
    "            {\n",
    "                \"bbox\": new_bbox,\n",
    "                \"label\": img2_annotations\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if self.transform:\n",
    "            merged_image = self.transform(merged_image)\n",
    "        else:\n",
    "            merged_image = transforms.ToTensor()(merged_image)\n",
    "            \n",
    "        annotations = torch.zeros(len(merged_annotations), 5)\n",
    "        \n",
    "        for i, ann in enumerate(merged_annotations):\n",
    "            annotations[i] = torch.cat((torch.tensor(ann[\"bbox\"]), torch.tensor([ann[\"label\"]])))\n",
    "            \n",
    "        return merged_image\n",
    "    \n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        image_width = int(root.find('size/width').text)\n",
    "        image_height = int(root.find('size/height').text)\n",
    "        \n",
    "        label = None\n",
    "        bbox = None\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text\n",
    "            \n",
    "            if label is None:\n",
    "                label = name\n",
    "                xmin = int(obj.find('bndbox/xmin').text)\n",
    "                ymin = int(obj.find('bndbox/ymin').text)\n",
    "                xmax = int(obj.find('bndbox/xmax').text)\n",
    "                ymax = int(obj.find('bndbox/ymax').text)\n",
    "                \n",
    "                bbox = [\n",
    "                    xmin / image_width,\n",
    "                    ymin / image_height,\n",
    "                    xmax / image_width,\n",
    "                    ymax / image_height,\n",
    "                ]\n",
    "        \n",
    "        label_num = 0 if label == 'cat' else 1 if label =='dog' else -1\n",
    "        \n",
    "        return label_num, torch.tensor(bbox, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "dataset = MyDataset(annotations_dir, image_dir, transform=transform)\n",
    "train_dataset, val_dataset = train_test_split(dataset, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleYOLO(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleYOLO, self).__init__()\n",
    "        self.backbone = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.backbone = nn.Sequential(*list(self.backbone.children())[:-2])\n",
    "        \n",
    "        self.fcs = nn.Linear(2048, 2*2*(4 + self.num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        features = F.adaptive_avg_pool2d(features, (1, 1))\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fcs(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "num_classes = 2\n",
    "class_to_idx = {\n",
    "    \"dog\": 0,\n",
    "    \"cat\": 1\n",
    "}\n",
    "\n",
    "model = SimpleYOLO(num_classes=num_classes).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(output, targets, device, num_classes):\n",
    "    mse_loss = nn.MSELoss()\n",
    "    ce_loss = nn.CrossEntropyLoss()\n",
    "    batch_size = output.shape[0]\n",
    "    total_los = 0\n",
    "    \n",
    "    output = output.view(batch_size, 2, 2, 4 + num_classes)\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        for j in range(len(targets[i])):\n",
    "            bbox_center_x = (targets[i][j][0] + targets[i][j][2]) / 2\n",
    "            bbox_center_y = (targets[i][j][1] + targets[i][j][3]) / 2\n",
    "            \n",
    "            grid_x = int(bbox_center_x*2)\n",
    "            grid_y = int(bbox_center_y*2)\n",
    "            \n",
    "            # 1. Classification loss\n",
    "            label_one_hot = torch.zeros(num_classes, device=device)\n",
    "            label_one_hot[int(targets[i][j][4])] = 1\n",
    "            \n",
    "            classification_loss = ce_loss(output[i, grid_y, grid_x, 4:], label_one_hot)\n",
    "            \n",
    "            # 2. Regression Loss for the responsible grid cell\n",
    "            bbox_target = targets[i][j][:4].to(device)\n",
    "            regression_loss = mse_loss(output[i, grid_y, grid_x, :4], bbox_target)\n",
    "            \n",
    "            # 3. No object Loss\n",
    "            no_obj_loss = 0\n",
    "            for other_grid_y in range(2):\n",
    "                for other_grid_x in range(2):\n",
    "                    if other_grid_y != grid_y or other_grid_x != grid_x:\n",
    "                        no_obj_loss += mse_loss(output[i, other_grid_y, other_grid_x, :4], torch.zeros(4, device=device))\n",
    "                        \n",
    "            total_loss += classification_loss + regression_loss + no_obj_loss\n",
    "            \n",
    "    return total_los / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, targets in tqdm(data_loader, desc=\"Validaton\", leave=False):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            output = model(images)\n",
    "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
    "            running_loss += total_loss.item()\n",
    "            \n",
    "            output = output.view(images.shape[0], 2, 2, 4 + num_classes)\n",
    "            \n",
    "            for batch_idx in range(images.shape[0]):\n",
    "                for target in targets[batch_idx]:\n",
    "                    bbox_center_x = (target[0] + target[2]) / 2\n",
    "                    bbox_center_y = (target[1] + target[3]) / 2\n",
    "                    grid_x = int(bbox_center_x*2)\n",
    "                    grid_y = int(bbox_center_y*2)\n",
    "\n",
    "                    prediction = output[batch_idx, grid_y, grid_x, :4].argmax().item()\n",
    "                    all_predictions.append(prediction)\n",
    "                    all_targets.append(target[4].item())\n",
    "        \n",
    "    val_loss = running_loss / len(data_loader)\n",
    "    \n",
    "    all_predictions = torch.tensor(all_predictions, device=device)\n",
    "    all_targets = torch.tensor(all_targets, device=device)\n",
    "    \n",
    "    val_accuracy = (all_predictions == all_targets).float().mean()\n",
    "    \n",
    "    return val_loss, val_accuracy.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, optimizer, num_epochs, device, num_classes):\n",
    "    best_val_accuracy = 0.0\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs), desc=\"Epochs\"):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for images, targets in tqdm(train_loader, desc=\"Batches\", leave=False):\n",
    "            images = images.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(images)\n",
    "            \n",
    "            total_loss = calculate_loss(output, targets, device, num_classes)\n",
    "            \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += total_loss.item()\n",
    "            \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(epoch_loss)\n",
    "        \n",
    "        val_loss, val_accuracy = evaluate_model(model, val_loader, device, num_classes)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_loss: .4f}\")\n",
    "        print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "        \n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            \n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, image_path, transform, device, class_to_idx, threshold=0.5):\n",
    "    model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
