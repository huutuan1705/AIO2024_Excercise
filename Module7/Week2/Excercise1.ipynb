{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting kagglehub\n",
      "  Downloading kagglehub-0.3.6-py3-none-any.whl.metadata (30 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kagglehub) (23.2)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from kagglehub) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kagglehub) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kagglehub) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->kagglehub) (2023.11.17)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm->kagglehub) (0.4.6)\n",
      "Downloading kagglehub-0.3.6-py3-none-any.whl (51 kB)\n",
      "   ---------------------------------------- 0.0/51.9 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 30.7/51.9 kB 660.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 51.9/51.9 kB 661.7 kB/s eta 0:00:00\n",
      "Installing collected packages: kagglehub\n",
      "Successfully installed kagglehub-0.3.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "data_dir = kagglehub.dataset_download(\"andrewmvd/dog-cat-detection\")\n",
    "print(\"Path to dataset file: \", data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import xml.etree.ElementTree as ET \n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision.models.resnet import ResNet18_Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, annotations_dir, image_dir, transform=None):\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_files = self.filter_images_with_multiple_objects()\n",
    "        \n",
    "    def filter_images_with_multiple_objects(self):\n",
    "        valid_image_files = []\n",
    "        for f in os.listdir(self.image_dir):\n",
    "            if os.path.isfile(os.path.join(self.image_dir, f)):\n",
    "                img_name = f\n",
    "                annotation_name = os.path.splitext(img_name)[0] +\".xml\"\n",
    "                annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "                \n",
    "                # Keep iomages that have single object\n",
    "                if self.count_objects_in_annotation(annotation_path) <= 1:\n",
    "                    valid_image_files.append(img_name)\n",
    "                else:\n",
    "                    print(f\"Image {img_name} has multiple objects and will be excluded from the dataset\")\n",
    "                \n",
    "    def count_objects_in_annotation(self, annotation_path):\n",
    "        try:\n",
    "            tree = ET.parse(annotation_path)\n",
    "            root = tree.getroot()\n",
    "            count = 0\n",
    "            for obj in root.findall(\"object\"):\n",
    "                count += 1\n",
    "        except FileNotFoundError:\n",
    "            return 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_files[idx]\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        \n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        annotation_name = os.path.splitext(img_name)[0] + \".xml\"\n",
    "        annotation_path = os.path.join(self.annotations_dir, annotation_name)\n",
    "        \n",
    "        label = self.parse_annotation(annotation_path)\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "    \n",
    "    def parse_annotation(self, annotation_path):\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        label = None\n",
    "        for obj in root.findall(\"object\"):\n",
    "            name = obj.find(\"name\").text\n",
    "            \n",
    "            if label is None:\n",
    "                label = name\n",
    "        \n",
    "        label_num = 0\n",
    "        return label_num     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_dir = os.path.join(data_dir, 'annotations')\n",
    "image_dir = os.path.join(data_dir, 'images')\n",
    "\n",
    "image_files = [f for f in os.listdir(image_dir) if os.path.isfile(os.path.join(image_dir, f))]\n",
    "\n",
    "df = pd.DataFrame({'image_name': image_files})\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = ImageDataset(annotations_dir, image_dir, transform=transform)\n",
    "val_dataset = ImageDataset(annotations_dir, image_dir, transform)\n",
    "\n",
    "train_dataset.image_files = [f for f in val_dataset.image_files if f in val_df['image_name'].values]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        scores = model(data)\n",
    "        loss = criterion(scores, targets)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for data, targets in val_loader:\n",
    "            data = data.to(device)\n",
    "            targets = targets.to(device)\n",
    "            scores = model(data)\n",
    "            _, predictions = scores.max(1)\n",
    "            correct += (predictions == targets).sum()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Validation Accuracy: {float(correct)/float(total)*100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
