{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from typing import List, Dict, Tuple\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing_Maccrobat:\n",
    "    def __init__(self, dataloader_folder, tokenizer):\n",
    "        self.file_ids = [f.split(\".\")[0] for f in os.listdir(dataloader_folder) if f.endswith('.txt')]\n",
    "        self.text_files = [f+\".txt\" for f in self.file_ids]\n",
    "        self.anno_files = [f+\".ann\" for f in self.file_ids]\n",
    "        self.num_samples = len(self.file_ids)\n",
    "        self.texts: List[str] = []\n",
    "        \n",
    "        for i in range(self.num_samples):\n",
    "            file_path = os.path.join(dataloader_folder, self.text_files[i])\n",
    "            with open(file_path, 'r') as f:\n",
    "                self.texts.append(f.read())\n",
    "                \n",
    "        self.tags: List[Dict[str, str]] = []\n",
    "        for i in range(self.num_samples):\n",
    "            file_path = os.path.join(dataloader_folder, self.anno_files[i])\n",
    "            with open(file_path, 'r') as f:\n",
    "                text_bound_ann = [t.split(\"\\t\") for t in f.read().split(\"\\n\") if t.startswith(\"T\")]\n",
    "                text_bound_lst = []\n",
    "                \n",
    "                for text_b in text_bound_ann:\n",
    "                    label = text_b[1].split(\" \")\n",
    "                    try:\n",
    "                        _ = int(label[1])\n",
    "                        _ = int(label[2])\n",
    "                        tag = {\n",
    "                            \"text\": text_b[-1],\n",
    "                            \"label\": label[0],\n",
    "                            \"start\": label[1],\n",
    "                            \"end\": label[2]\n",
    "                        }\n",
    "                        text_bound_lst.append(tag)\n",
    "                    except:\n",
    "                        pass\n",
    "                \n",
    "                self.tags.append(text_bound_lst)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def process(self) -> Tuple[List[List[str]], List[List[str]]]:\n",
    "        input_texts = []\n",
    "        input_labels = []\n",
    "        \n",
    "        for idx in range(self.num_samples):\n",
    "            full_text = self.texts[idx]\n",
    "            tags = self.tags[idx]\n",
    "            \n",
    "            label_offset = []\n",
    "            continuous_label_offset = []\n",
    "            for tag in tags:\n",
    "                offset = list(range(int(tag[\"start\"]), int(tag[\"end\"])+1))\n",
    "                label_offset.append(offset)\n",
    "                continuous_label_offset.extend(offset)\n",
    "                \n",
    "            all_offset = list(range(len(full_text)))\n",
    "            zero_offset = [offset for offset in all_offset if offset not in continuous_label_offset]\n",
    "            zero_offset = Preprocessing_Maccrobat.find_continous_ranges(zero_offset)\n",
    "            \n",
    "            self.tokens = []\n",
    "            self.labels = []\n",
    "            self._merge_offset(full_text, tags, zero_offset, label_offset)\n",
    "            assert len(self.tokens) == len(self.labels), f\"Length of tokens and labels are not equal\"\n",
    "            \n",
    "            input_texts.append(self.tokens)\n",
    "            input_labels.append(self.labels)\n",
    "            \n",
    "        return input_texts, input_labels\n",
    "            \n",
    "    def _merge_offset(self, full_text, tags, zero_offset, label_offset):\n",
    "        i = j = 0 \n",
    "        while i < len(zero_offset) and j < len(label_offset):\n",
    "            if zero_offset[i][0] < label_offset[j][0]:\n",
    "                self._add_zero(full_text, zero_offset, i)\n",
    "                i += 1\n",
    "            else:\n",
    "                self._add_label(full_text, label_offset, j, tags)\n",
    "                j += 1\n",
    "        \n",
    "        while i < len(zero_offset):\n",
    "            self._add_zero(full_text, zero_offset, i)\n",
    "            i += 1\n",
    "        \n",
    "        while j < len(label_offset):\n",
    "            self._add_label(full_text, label_offset, j, tags)\n",
    "            j += 1\n",
    "    \n",
    "    def _add_zero(self, full_text, offset, index):\n",
    "        start, *_, end = offset[index] if len(offset[index]) > 1 else (offset[index][0], offset[index][0] + 1)\n",
    "        text = full_text[start:end]\n",
    "        text_tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        self.tokens.extend(text_tokens)\n",
    "        self.labels.extend(\n",
    "            [\"0\"]*len(text_tokens)\n",
    "        )\n",
    "        \n",
    "    def _add_label(self, full_text, offset, index, tags):\n",
    "        start, *_, end = offset[index] if len(offset[index]) > 1 else (offset[index][0], offset[index][0] + 1)\n",
    "        text = full_text[start:end]\n",
    "        text_tokens = self.tokenizer.tokenize(text)\n",
    "        \n",
    "        self.tokens.extend(text_tokens)\n",
    "        self.labels.extend(\n",
    "            [f\"B-{tags[index]['label']}\"] + [f\"I-{tags[index]['label']}\"]*(len(text_tokens)-1)\n",
    "        )\n",
    "        \n",
    "    @staticmethod\n",
    "    def build_label2id(tokens: List[List[str]]):\n",
    "        label2id = {}\n",
    "        id_counter = 0\n",
    "        for token in [token for sublist in tokens for token in sublist]:\n",
    "            if token not in label2id:\n",
    "                label2id[token] = id_counter\n",
    "                id_counter += 1\n",
    "        return label2id\n",
    "        \n",
    "    @staticmethod\n",
    "    def find_continous_ranges(data: List[int]):\n",
    "        if not data:\n",
    "            return []\n",
    "        ranges = []\n",
    "        start = []\n",
    "        prev = []      \n",
    "        for number in data[1:]:\n",
    "            if number != prev + 1:\n",
    "                ranges.append(list(range(start, prev+1)))\n",
    "                start = number\n",
    "            prev = number\n",
    "        ranges.append(list(range(start, prev+1)))\n",
    "        return ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/biomedical-ner-all\")\n",
    "dataset_folder = \"./MACCROBAT2018\"\n",
    "\n",
    "maccrobat_builder = Preprocessing_Maccrobat(dataset_folder, tokenizer)\n",
    "input_texts, input_labels = maccrobat_builder.process()\n",
    "\n",
    "label2id = Preprocessing_Maccrobat.build_label2id(input_labels)\n",
    "id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_train, inputs_val, labels_train, labels_val = train_test_split(\n",
    "    input_texts,\n",
    "    input_labels,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 512\n",
    "\n",
    "class NER_Dataset(Dataset):\n",
    "    def __init__(self, input_texts, input_labels, tokenizer, label2id, max_len=MAX_LEN):\n",
    "        super().__init__()\n",
    "        self.tokens = input_texts\n",
    "        self.labels = input_labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label2id = label2id\n",
    "        self.max_len = max_len\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_token = self.tokens[idx]\n",
    "        label_token = [self.label2id[label] for label in self.labels[idx]]\n",
    "        \n",
    "        input_token = self.tokenizer.convert_tokens_to_idx(input_token)\n",
    "        attention_mask = [1]*len(input_token)\n",
    "        \n",
    "        input_ids = self.pad_and_truncate(input_token, pad_id=self.tokenizer.pad_token_id)\n",
    "        labels = self.pad_and_truncate(label_token, pad_id=0)\n",
    "        attention_mask = self.pad_and_truncate(attention_mask, pad_id=0)\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": torch.as_tensor(input_ids),\n",
    "            \"labels\": torch.as_tensor(labels),\n",
    "            \"attention_mask\": torch.as_tensor(attention_mask)\n",
    "        }\n",
    "        \n",
    "    def pad_and_truncate(self, inputs: List[int], pad_id: int):\n",
    "        if len(inputs) < self.max_len:\n",
    "            padded_inputs = inputs + [pad_id] * (self.max_len - len(inputs))\n",
    "        else:\n",
    "            padded_inputs = inputs[:self.max_len]\n",
    "        return padded_inputs\n",
    "    \n",
    "    def label2id(self, labels: List[str]):\n",
    "        return [self.label2id[label] for label in labels]\n",
    "    \n",
    "train_set = NER_Dataset(inputs_train, labels_train, tokenizer, label2id)\n",
    "val_set = NER_Dataset(inputs_val, labels_val, tokenizer, label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"d4data/biomedical-ner-all\",\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    mask = labels != 0\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions[mask], references=labels[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"output_dir\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=20,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    optim=\"adamw_torch\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_set,\n",
    "    eval_dataset=val_set,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
